---
layout:     post
title:        爬~虫
subtitle:    简单爬虫的两种方式
date:       2018-05-23
author:     CDX
header-img: img/post-bg-re-vs-ng2.jpg
catalog: true
tags:
    - Aichen
---
## 最近要搞课程设计和小伙伴想弄一个分布式爬虫，一个master和多个node，先确定一些简单的爬取方法。
Python实现
模块：Flask、Twisted、requests、bs4、jieba、wordcloud、selenium
扩展：谷歌浏览器驱动
## 爬取方式1
```
    headers = {'User-Agent': 'Mozilla/5.0'}
    def get_whole_url_page(self, url):
        '''
        抓取页面全部的html
        :param url:
        :return:
        '''
        try:
            r = requests.get(url,self.headers)
            bf1 = BeautifulSoup(r.content, "html.parser")
        except TypeError:
            bf1 = None
            exit(-1)
        return bf1
```
## 爬取方式2
```
@monitor_timer
    def get_ajax_page(self, url):
        options = webdriver.ChromeOptions()
        options.add_argument(
            'user-agent="Mozilla/5.0 (Linux; Android 4.0.4; Galaxy Nexus Build/IMM76B) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.133 Mobile Safari/535.19"')
        driver = webdriver.Chrome(chrome_options=options)
        driver.get(url)

        html = driver.page_source
        return html
```
## 递归爬取
```
@monitor_timer
    def collect_num_page(self, url, num):
        '''
        进行广度遍历抓取页面html，num为随机个数,num为零是默认为全部网页
        :param url:
        :param num:
        :return:
        '''
        result_href = self.find_flow_href(url)
        result_html = {}
        print result_href
        if num < len(result_href):
            pass
        else:
            num = len(result_href)
        a = [random.randint(0, len(result_href)-1) for __ in range(num)]
        for line in a:
            try:
                result_html[str(result_href[line])] = self.get_whole_url_page(
                    result_href[line]).text
                logger.info(result_href[line] + " Finished!")
            except BaseException:
                result_html[str(result_href[line])] = self.get_ajax_page(
                    result_href[line]).te
                logger.info(result_href[line] + " Finished!")
            finally:
                pass
                # logger.info(result_href[line] + " Failed!")
        return result_html
```