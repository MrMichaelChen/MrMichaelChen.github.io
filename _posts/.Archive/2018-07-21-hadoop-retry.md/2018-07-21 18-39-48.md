---
layout:     post
title:      Hadoop组件复习
subtitle:   再会Hadoop、Hdfs、Hive、Hbase、Sqoop、Flume
date:       2018-07-21
author:     陈德玺
header-img: img/post-bg-kuaidi.jpg
catalog: true
tags:
    - Hadoop
---
## HADOOP
最近投了简历，正在抓紧复习大数据的知识，重新安装部署了单节点的**Hadoop**环境，练练手。

## Hadoop
- 1、start-all.sh
        开启所有服务，启动namenode，datanode，jps，yarn等服务
- 2、yarn工作方式
## Hdfs
- 1、DataNode初始化问题
- 2、fs基本命令
- 3、文件操作（压缩）
## Hive
- 1、部署
- 2、mysql元数据库
- 3、初始化元数据库
## Hbase
- 1、部署
- 2、添加host防止timeout
- 3、hbase基本命令
## Sqoop
- 1、部署
- 2、解决缺少组件问题
- 3、hive文件存储位置
- 4、与mysql互导
## Flume
- 1、部署
- 2、配置环境变量
- 3、监控port
- 4、监控本地文件

# Eclipse编程练习

## Hive ORM

```
package hadoop_text;

import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.ResultSet;
import java.sql.SQLException;
import java.sql.Statement;

import org.apache.log4j.Logger;

public class HiveJdbcCli {
	private static String driverName = "org.apache.hive.jdbc.HiveDriver";
	private static String url = "jdbc:hive2://192.168.1.107:10000/test";
	private static String user = "root";
	private static String password = "123456";
	private static String sql = "";
	private static ResultSet res;
	private static final Logger log = Logger.getLogger(HiveJdbcCli.class);

	public static void main(String[] args) throws Exception {
		Connection conn = null;
		Statement stmt = null;
		try {
			conn = getConn();
			stmt = conn.createStatement();
			// dropTable(stmt);
			// showTables(stmt,"cdx");
			// describeTables(stmt,"cdx");
			// createTable(stmt,"cdx");
			// loadData(stmt,"cdx");
			// selectData(stmt,"cdx");
			// countData(stmt,"cdx");
			// stmt.execute("insert into cdx values(1,2)");
			System.out.println("finish!");
		} catch (ClassNotFoundException e) {
			e.printStackTrace();
			System.exit(1);
		} catch (SQLException e) {
			e.printStackTrace();
			System.exit(1);
		} finally {
			try {
				if (conn != null) {
					conn.close();
					conn = null;
				}
				if (stmt != null) {
					// 此处的关闭会有报错，但不影响程序运行，挖坑待解决
					stmt.close();
					stmt = null;
				}
			} catch (SQLException e) {
				e.printStackTrace();
			}
		}
	}

	private static void selectData(Statement stmt, String tableName) throws SQLException {
		// TODO Auto-generated method stub
		sql = "select * from " + tableName;
		System.out.println("Running " + sql);
		res = stmt.executeQuery(sql);
		System.out.println("select执行结果：");
		while (res.next()) {
			System.out.println(res.getInt(1) + "\t" + res.getString(2));
		}
	}

	private static void describeTables(Statement stmt, String tableName) throws SQLException {
		// TODO Auto-generated method stub
		sql = "describe " + tableName;
		System.out.println("Running: " + sql);
		res = stmt.executeQuery(sql);
		System.out.println("describe执行结果:");
		if (res.next()) {
			System.out.println(res.getString(1));
		}
	}

	private static void countData(Statement stmt, String tableName) throws SQLException {
		// TODO Auto-generated method stub
		sql = "select count(1) from " + tableName;
		System.out.println("Running:" + sql);
		res = stmt.executeQuery(sql);
		System.out.println("count运行结果如下：");
		while (res.next()) {
			System.out.println("count ---->" + res.getString(1));
		}
	}

	private static void showTables(Statement stmt, String tableName) throws SQLException {
		// TODO Auto-generated method stub
		sql = "show tables '" + tableName + "'";
		System.out.println("Running:" + sql);
		res = stmt.executeQuery(sql);
		System.out.println("执行 show tables 的结果:");
		if (res.next()) {
			System.out.println(res.getString(1));
		}
	}

	private static void createTable(Statement stmt, String tableName) throws SQLException {
		// TODO Auto-generated method stub
		sql = "create table " + tableName + " (key int, value string) row format delimited fields terminated by ','";
		stmt.execute(sql);
	}

	private static void dropTable(Statement stmt) throws SQLException {
		// TODO Auto-generated method stub
		String tableName = "cdx";
		sql = "drop table " + tableName;
		stmt.execute(sql);
	}

	private static Connection getConn() throws ClassNotFoundException, SQLException {
		Class.forName(driverName);
		Connection conn = DriverManager.getConnection(url, user, password);
		return conn;
	}

	private static void loadData(Statement stmt, String tableName) throws SQLException {
		String filepath = "/data";
		sql = "load data local inpath '" + filepath + "' into table " + tableName;
		System.out.println("load Running:" + sql);
		stmt.execute(sql);
	}

}
```

## Hbase ORM

```
package hadoop_text;

import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.HColumnDescriptor;
import org.apache.hadoop.hbase.HTableDescriptor;
import org.apache.hadoop.hbase.KeyValue;
import org.apache.hadoop.hbase.MasterNotRunningException;
import org.apache.hadoop.hbase.ZooKeeperConnectionException;
import org.apache.hadoop.hbase.client.Delete;
import org.apache.hadoop.hbase.client.Durability;
import org.apache.hadoop.hbase.client.Get;
import org.apache.hadoop.hbase.client.HBaseAdmin;
import org.apache.hadoop.hbase.client.HTable;
import org.apache.hadoop.hbase.client.Put;
import org.apache.hadoop.hbase.client.Result;
import org.apache.hadoop.hbase.client.ResultScanner;
import org.apache.hadoop.hbase.client.Scan;
import org.apache.hadoop.hbase.thrift.generated.Hbase.AsyncProcessor.createTable;
import org.apache.hadoop.hbase.util.Bytes;
import org.apache.log4j.Logger;

import com.sun.org.apache.xalan.internal.xsltc.compiler.sym;

public class HbaseDao {
	public static Logger log = Logger.getLogger(HbaseDao.class);
	public static Configuration configuration;
	static {
		configuration = HBaseConfiguration.create();
		configuration.set("hbase.zookeeper.quorum", "192.168.1.107");
		// configuration.set("hbase_zookeeper_quorum", "ZooKeeper1,ZooKeeper2");
		configuration.set("hbase.zookeeper.property.clientPort", "2181");
		configuration.set("hbase.master", "192.168.1.107:6000");
	}

	public static void main(String[] args) throws IOException {
		createTable("Cdx_Data");
		// System.out.println(11);
		// dropTable("Cdx_Data");

		insertData("cdx", "Cdx_Data", "column1");
		// insertData("sxc","cdx");

		QueryAll("Cdx_Data");
		// QueryByCondition1("cdx","Cdx_Data");
		// QueryBycondition1("xsc","Cdx_Data");

		// deleteRow("Cdx_Data","cdx");
		System.out.println("Finish！");
	}

	// 创建表格
	public static void createTable(String tableName) throws IOException {
		// TODO Auto-generated method stub
		log.info("start create table .....");
		try {
			HBaseAdmin hBaseadmin = new HBaseAdmin(configuration);
			if (hBaseadmin.tableExists(tableName)) {
				// 如果存在数据库 先删除在创建 爱晨
				hBaseadmin.disableTable(tableName);
				hBaseadmin.deleteTable(tableName);
				log.info("------------->" + tableName + "is exist,delete....");
			}
			HTableDescriptor tableDescriptor = new HTableDescriptor(tableName);
			// 添加数据family
			tableDescriptor.addFamily(new HColumnDescriptor("column1"));
			tableDescriptor.addFamily(new HColumnDescriptor("column2"));
			tableDescriptor.addFamily(new HColumnDescriptor("column3"));
			hBaseadmin.createTable(tableDescriptor);
		} catch (MasterNotRunningException e) {
			e.printStackTrace();
		} catch (ZooKeeperConnectionException e) {
			e.printStackTrace();
		} catch (IOException e) {
			e.printStackTrace();
		}
	}

	// 插入数据 变量后期定义
	private static void insertData(String rowkey, String tableName, String columnname) {
		try {
			HTable ht = new HTable(configuration, tableName);

			// row key
			Put put = new Put(rowkey.getBytes());

			put.add(Bytes.toBytes(columnname), "screen_width".getBytes(), "1080".getBytes());
			put.add(Bytes.toBytes(columnname), "screen_height".getBytes(), "1092".getBytes());
			put.add(Bytes.toBytes(columnname), "url".getBytes(), "www.baidu.com".getBytes());
			put.add(Bytes.toBytes(columnname), "event_data".getBytes(), "12|16|18".getBytes());

			put.setDurability(Durability.SYNC_WAL);

			ht.put(put);
			ht.close();
		} catch (IOException e) {
			e.printStackTrace();
		}
	}

	// 删除数据表格
	public static void dropTable(String tableName) {
		try {
			HBaseAdmin admin = new HBaseAdmin(configuration);
			admin.disableTable(tableName);
			admin.deleteTable(tableName);
			log.info("------------->" + tableName + "is exist,delete....");
		} catch (MasterNotRunningException e) {
			e.printStackTrace();
		} catch (ZooKeeperConnectionException e) {
			e.printStackTrace();
		} catch (IOException e) {
			e.printStackTrace();
		}
	}

	// 删除表中的一条记录
	public static void deleteRow(String tablename, String rowkey) {
		// TODO Auto-generated method stub
		try {
			HTable table = new HTable(configuration, tablename);
			List list = new ArrayList();
			Delete d1 = new Delete(rowkey.getBytes());
			list.add(d1);

			table.delete(list);
			System.out.println("--------->" + rowkey + "删除成功");
		} catch (IOException e) {
			e.printStackTrace();
		}
	}

	// 查询所有数据
	public static void QueryAll(String tableName) {
		try {
			HTable table = new HTable(configuration, tableName);
			ResultScanner rs = table.getScanner(new Scan());
			for (Result r : rs) {
				System.out.println("获得到rowkey:" + new String(r.getRow()));
				for (KeyValue keyValue : r.raw()) {
					System.out.println(
							"列" + new String(keyValue.getFamily()) + "====值：" + new String(keyValue.getValue()));
				}
			}
		} catch (IOException e) {
			e.printStackTrace();
		}
	}

	// 单条件查询 根据rowkey查询值
	public static void QueryByCondition1(String rowkey, String tableName) {
		try {
			HTable table = new HTable(configuration, tableName);
			Get scan = new Get(rowkey.getBytes());
			Result r = table.get(scan);
			System.out.println("获得rowkey：" + new String(r.getRow()));
			for (KeyValue keyvalue : r.raw()) {
				System.out.println("列:" + new String(keyvalue.getFamily()) + "===值:" + new String(keyvalue.getValue()));
			}
		} catch (IOException e) {
			e.printStackTrace();
		}
	}
}

```

## KafkaProducer

```
package hadoop_text;

import com.oracle.jrockit.jfr.Producer;

public class kafkaProducer extends Thread {
	private String topic;

	public kafkaProducer(String topic) {
		super();
		this.topic = topic;
	}

	@Override
	public void run() {
		Producer producer = createProducer();
		int i = 0;
		while (true) {
			try {
				// String path = "D:\\env\\data\\1.txt"
				System.out.println("111");
			} finally {
				System.out.println("1");
			}
		}
	}

	private Producer createProducer() {
		// TODO Auto-generated method stub
		return null;
	}
}
```